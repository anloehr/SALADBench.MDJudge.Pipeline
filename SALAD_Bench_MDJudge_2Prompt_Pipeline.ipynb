{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anloehr/SALADBench.MDJudge.Pipeline/blob/main/SALAD_Bench_MDJudge_2Prompt_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r1v7rmPVVWBK"
      },
      "outputs": [],
      "source": [
        "# Set up https://github.com/OpenSafetyLab/SALAD-BENCH/  and install dependencies\n",
        "!git clone https://github.com/OpenSafetyLab/SALAD-BENCH.git\n",
        "!cd SALAD-BENCH\n",
        "!pip install transformers==4.36.1\n",
        "!pip install ray\n",
        "!pip install flash-attn\n",
        "!pip install vllm\n",
        "!pip install -e .\n",
        "!pip install datasets\n",
        "!pip install openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ApnCQaIMf4Mx"
      },
      "outputs": [],
      "source": [
        "# Check if we have a GPU set up\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2TtGi5rHcKLU"
      },
      "outputs": [],
      "source": [
        "# Change directory to execution directory\n",
        "\n",
        "import os\n",
        "os.chdir(\"SALAD-BENCH\")\n",
        "import datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crr840tMZrvj"
      },
      "outputs": [],
      "source": [
        "# Set up OpenAI API call with private key read from local directory\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from pprint import pprint\n",
        "from google.colab import files\n",
        "\n",
        "# This will pop up a file‚Äêpicker; select your private OpenAi API key\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read it into a string\n",
        "filename = list(uploaded.keys())[0]\n",
        "key = uploaded[filename].decode(\"utf-8\").strip()\n",
        "\n",
        "client = OpenAI(api_key=key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MC3g2qRPpBBn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# Load the SALAD-Bench base_set\n",
        "from datasets import load_dataset\n",
        "\n",
        "salad = load_dataset(\"OpenSafetyLab/Salad-Data\", \"base_set\")\n",
        "\n",
        "ds = salad[\"train\"]\n",
        "\n",
        "# define prefix to SALAD question for prompt type change\n",
        "# 1) Please answer the question:\n",
        "# 2) Please answer the question. Think through it step by step and ELI5 me through your reasoning:\n",
        "PREFIX = (\n",
        "    \"Please answer the question:\"\n",
        ")\n",
        "\n",
        "\n",
        "# map to add new field in the dataset\n",
        "ds = ds.map(lambda ex: {\"prompt\": PREFIX + ex[\"question\"]})\n",
        "\n",
        "# typecast ds to list of strings\n",
        "questions = ds[\"prompt\"]\n",
        "#print(questions)\n",
        "\n",
        "# Generate responses from LLM\n",
        "responses = []\n",
        "# Call the API for each example in small and collect the outputs\n",
        "for prompt in questions:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "    # automatically a string list\n",
        "    responses.append(answer)\n",
        "\n",
        "    # Space the API requests to avoid crashes\n",
        "    time.sleep(0.5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2uNLoQhbEcp"
      },
      "outputs": [],
      "source": [
        "print(len(responses))\n",
        "print(len(questions))\n",
        "\n",
        "# Export questions, responses, and harm categories to csv files\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Define the path to the RESULTS directory\n",
        "results_dir = \"/content/SALAD_BENCH_Results/Prompt1/\"\n",
        "\n",
        "\n",
        "# Create the RESULTS directory if it doesn't exist\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "    print(f\"Created directory: {results_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {results_dir}\")\n",
        "\n",
        "# 1. Export Questions to CSV in the RESULTS directory\n",
        "questions_file = os.path.join(results_dir, \"questions.csv\")\n",
        "with open(questions_file, 'w', newline='', encoding='utf-8') as q_csvfile:\n",
        "    writer = csv.writer(q_csvfile)\n",
        "    writer.writerow([\"Index\", \"Question\"])  # Write header\n",
        "    for index, question in enumerate(questions):\n",
        "        writer.writerow([index, question])\n",
        "\n",
        "print(f\"Questions exported to {questions_file}\")\n",
        "\n",
        "# 2. Export Responses to CSV in the RESULTS directory\n",
        "responses_file = os.path.join(results_dir, \"responses.csv\")\n",
        "with open(responses_file, 'w', newline='', encoding='utf-8') as r_csvfile:\n",
        "    writer = csv.writer(r_csvfile)\n",
        "    writer.writerow([\"Index\", \"Response\"])  # Write header\n",
        "    for index, response in enumerate(responses):\n",
        "        writer.writerow([index, response])\n",
        "\n",
        "print(f\"Responses exported to {responses_file}\")\n",
        "\n",
        "# 3. Export Categories to CSV in the RESULTS directory\n",
        "categories_file = os.path.join(results_dir, \"categories.csv\")\n",
        "with open(categories_file, 'w', newline='', encoding='utf-8') as c_csvfile:\n",
        "    writer = csv.writer(c_csvfile)\n",
        "    writer.writerow([\"Index\", \"1-Category\", \"2-Category\", \"3-Category\"])  # Write header\n",
        "    for index, example in enumerate(small):\n",
        "        writer.writerow([index, example[\"1-category\"], example[\"2-category\"], example[\"3-category\"]])\n",
        "\n",
        "print(f\"Categories exported to {categories_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbbCX7AhCzpm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NCxWuWeIZzt-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"SALAD-BENCH\")\n",
        "\n",
        "# Set up evaluator model MD-Judge\n",
        "from saladbench.evaluator import MDJudgeEvaluator\n",
        "\n",
        "model_ckpt = \"OpenSafetyLab/MD-Judge-v0.1\"\n",
        "judge = MDJudgeEvaluator(\n",
        "   model_ckpt, use_vllm=False, use_flash_attn=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LIT0hHBoK9QE"
      },
      "outputs": [],
      "source": [
        "# The Meat: running MDJudgeEvaluator on the questions and responses\n",
        "\n",
        "probs_result = judge.eval_unsafe_probs(questions, responses)\n",
        "safety_result = judge.eval_unsafe_labels(questions, responses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPqmzgwzUqJW"
      },
      "outputs": [],
      "source": [
        "# Export probs_result, safety_result to multiple csv files\n",
        "import csv\n",
        "\n",
        "# Define the path to the RESULTS directory\n",
        "results_dir = \"/content/SALAD_BENCH_Results/Prompt1/\"\n",
        "\n",
        "# Create the RESULTS directory if it doesn't exist\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)\n",
        "    print(f\"Created directory: {results_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {results_dir}\")\n",
        "\n",
        "# Export Evaluation Results to CSV in the RESULTS directory\n",
        "results_file = os.path.join(results_dir, \"evaluation_results.csv\")\n",
        "with open(results_file, 'w', newline='', encoding='utf-8') as res_csvfile:\n",
        "    writer = csv.writer(res_csvfile)\n",
        "    writer.writerow([\"Index\", \"unsafe_probability\", \"unsafe_label_numeric\", \"unsafe_label_string\"])  # Write header\n",
        "    for index in range(len(probs_result)):\n",
        "        writer.writerow([index, probs_result[index], safety_result[0][index], safety_result[1][index]])\n",
        "\n",
        "print(f\"Evaluation results exported to {results_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS-sxKmNWuW2"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olwMK9La7PM6"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}